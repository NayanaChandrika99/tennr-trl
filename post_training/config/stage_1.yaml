model: HuggingFaceTB/SmolLM2-135M-Instruct
tokenizer: HuggingFaceTB/SmolLM2-135M-Instruct
dataset:
  type: local
  path: data/processed/stage_1.jsonl
  format: jsonl
  split: train
chat_template: |
  {% for message in messages %}
    {% if loop.first and messages[0]['role'] != 'system' %}
      {{ '<|im_start|>system\nYou are a helpful AI assistant named Tiny Reasoning Language Model, trained by Shekswess.<|im_end|>\n' }}
    {% endif %}
    {{ '<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>\n' }}
  {% endfor %}
  {% if add_generation_prompt %}
    {{ '<|im_start|>assistant\n' }}
  {% endif %}
trainer:
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 1
  gradient_checkpointing: true
  num_train_epochs: 1
  max_steps: 5
  learning_rate: 0.0003
  weight_decay: 0.0
  warmup_ratio: 0.0
  max_grad_norm: 1.0
  loss_type: dft
  save_strategy: "epoch"
  logging_steps: 1
  log_level: "info"
  report_to: "none"
  run_name: "thce-stage-1-sft"
  output_dir: "outputs/stage_1"
  push_to_hub: false
  max_length: 2048
  lr_scheduler_type: cosine
  dataloader_drop_last: false
  dataloader_persistent_workers: false
  dataloader_pin_memory: false
  dataloader_num_workers: 0
  packing: false
  disable_tqdm: false
  neftune_noise_alpha: 0.0
