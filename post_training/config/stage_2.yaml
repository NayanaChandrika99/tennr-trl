model: outputs/stage_1/checkpoint-5  # path to Stage 1 checkpoint once trained
tokenizer: outputs/stage_1/checkpoint-5
dataset:
  type: local
  path: data/processed/stage_2.jsonl
  format: jsonl
  split: train
chat_template: |
  {% for message in messages %}
    {% if loop.first and messages[0]['role'] != 'system' %}
        {{ '<|im_start|>system\nYou are a helpful AI assistant named Tiny Reasoning Language Model, trained by Shekswess. You are an assistant, with the ability to do reasoning. When performing reasoning always perform your full chain of thought inside <think>...</think> before giving a final answer.<|im_end|>\n' }}
    {% endif %}
    {{ '<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>\n' }}
  {% endfor %}
  {% if add_generation_prompt %}
      {{ '<|im_start|>assistant\n' }}
  {% endif %}
tokenizer_additional_special_tokens:
  - "<think>"
  - "</think>"
trainer:
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 1
  gradient_checkpointing: true
  num_train_epochs: 1
  max_steps: 5
  learning_rate: 0.0003
  weight_decay: 0.0
  warmup_ratio: 0.0
  max_grad_norm: 1.0
  save_strategy: "steps"
  save_steps: 5
  logging_steps: 1
  log_level: "info"
  report_to: "none"
  run_name: "thce-stage-2-sft"
  output_dir: "outputs/stage_2"
  push_to_hub: false
  max_length: 2048
  lr_scheduler_type: cosine
  dataloader_drop_last: false
  dataloader_persistent_workers: false
  dataloader_pin_memory: false
  dataloader_num_workers: 0
  packing: false
  disable_tqdm: false
  neftune_noise_alpha: 0.0
