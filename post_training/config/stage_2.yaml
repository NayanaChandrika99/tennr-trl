model: Shekswess/trlm-stage-1-sft-final-2
tokenizer: Shekswess/trlm-stage-1-sft-final-2
dataset:
  name: "Shekswess/trlm-sft-stage-2-final-2" # around 60k samples of reasoning data
  split: "train"
  num_proc: 16
chat_template: |
  {% for message in messages %}
    {% if loop.first and messages[0]['role'] != 'system' %}
        {{ '<|im_start|>system\nYou are a helpful AI assistant named Tiny Reasoning Language Model, trained by Shekswess. You are an assistant, with the ability to do reasoning. When performing reasoning always perform your full chain of thought inside <think>...</think> before giving a final answer.<|im_end|>\n' }}
    {% endif %}
    {{ '<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>\n' }}
  {% endfor %}
  {% if add_generation_prompt %}
      {{ '<|im_start|>assistant\n' }}
  {% endif %}
tokenizer_additional_special_tokens:
  - "<think>"
  - "</think>"
trainer:
  per_device_train_batch_size: 32
  gradient_accumulation_steps: 4
  gradient_checkpointing: true
  num_train_epochs: 1
  learning_rate: 0.0003
  weight_decay: 0.02
  warmup_ratio: 0.1
  max_grad_norm: 1.0
  save_strategy: "steps"
  save_steps: 300
  logging_steps: 1
  log_level: "info"
  report_to: "wandb"
  run_name: "trlm-stage-2-sft-final-2"
  output_dir: "outputs/stage_2"
  push_to_hub: true
  hub_model_id: "Shekswess/trlm-stage-2-sft-final-2"
  hub_strategy: "every_save"
  max_length: 4096
  lr_scheduler_type: cosine
  dataloader_drop_last: true
  dataloader_persistent_workers: true
  dataloader_pin_memory: true
  dataloader_num_workers: 12
  packing: false
  disable_tqdm: false
  neftune_noise_alpha: 0.02